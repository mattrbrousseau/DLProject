@misc{Schoonjans,
author = {Schoonjans, Frank},
booktitle = {May 09, 2017},
mendeley-groups = {DeepLearning/Audio},
title = {{ROC Curve Analysis with MedCalc}},
url = {https://www.medcalc.org/manual/roc-curves.php},
urldate = {July 26, 2018}
}
@article{Eghbal-Zadeh,
author = {Eghbal-Zadeh, H and Lehner, B and {\ldots}, M Dorfer - IEEE AASP Challenge and undefined 2016},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eghbal-Zadeh et al. - Unknown - CP-JKU submissions for DCASE-2016 A hybrid approach using binaural i-vectors and deep convolutional neur.pdf:pdf},
journal = {researchgate.net},
mendeley-groups = {DeepLearning,DeepLearning/Audio},
title = {{CP-JKU submissions for DCASE-2016: A hybrid approach using binaural i-vectors and deep convolutional neural networks}},
url = {https://www.researchgate.net/profile/Bernhard{\_}Lehner/publication/306011374{\_}CP-JKU{\_}Submissions{\_}for{\_}DCASE-2016{\_}a{\_}Hybrid{\_}Approach{\_}Using{\_}Binaural{\_}I-Vectors{\_}and{\_}Deep{\_}Convolutional{\_}Neural{\_}Networks/links/57bd4f0808ae6c703bc5eaa7.pdf}
}
@misc{,
mendeley-groups = {DeepLearning/Audio},
title = {{Sound. Wikipedia}},
url = {https://en.wikipedia.org/wiki/Sound},
urldate = {2018-07-15}
}
@misc{,
mendeley-groups = {DeepLearning/Audio},
title = {{General-purpose Audio Tagging of Freesound Content with AudioSet Labels - DCASE}},
url = {http://dcase.community/challenge2018/task-general-purpose-audio-tagging},
urldate = {2018-07-19}
}
@misc{,
mendeley-groups = {DeepLearning/Audio},
title = {{LibROSA - Librosa 0.6.0 Documentation}},
url = {https://librosa.github.io/librosa/index.html},
urldate = {2018-07-19}
}
@misc{,
mendeley-groups = {DeepLearning/Audio},
title = {{The Components of Sound}},
url = {https://www.nde-ed.org/EducationResources/HighSchool/Sound/components.htm},
urldate = {2018-07-19}
}
@techreport{Yun2016,
abstract = {This report describes the algorithm for audio scene classification and audio tagging and the result for DCASE 2016 challenge data. We propose a discriminative training algorithm to improve the baseline GMM performance. The algorithm updates the baseline GMM parameters by maximizing the margin between classes to improve discriminative performance. For Task1, we use a hierarchical classifier to maximize discriminative performance, and achieve 84{\%} accuracy for given cross validation data. For Task4, we apply binary classifier for each label, and achieve 16.71{\%} EER for given cross validation data.},
author = {Yun, Sungrack and Kim, Sungwoong and Moon, Sunkuk and Cho, Juncheol and Kim, Taesu},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yun et al. - 2016 - Detection and Classification of Acoustic Scenes and Events.pdf:pdf},
keywords = {Gaussian mixture model (GMM),Index Terms-audio scene classification,audio tag-ging,discriminative training,multi-label classification},
mendeley-groups = {DeepLearning,DeepLearning/Audio},
title = {{Discriminative training of gmm parameters for audio scene classification and audio tagging}},
url = {http://www.cs.tut.fi/sgn/arg/dcase2016/documents/challenge{\_}technical{\_}reports/DCASE2016{\_}Kim{\_}1006.pdf},
year = {2016}
}
@article{Choi2016,
abstract = {We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D con-volutional layers and subsampling layers only. In the experiments , we measure the AUC-ROC scores of the archi-tectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.00298v1},
author = {Choi, Keunwoo and Fazekas, Gy{\"{o}}rgy and Sandler, Mark},
eprint = {arXiv:1606.00298v1},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Fazekas, Sandler - Unknown - AUTOMATIC TAGGING USING DEEP CONVOLUTIONAL NEURAL NETWORKS.pdf:pdf},
mendeley-groups = {DeepLearning/Audio},
title = {{Automatic tagging using deep convolutional neural networks}},
url = {https://arxiv.org/pdf/1606.00298.pdf},
year = {2016}
}
@article{LeeJiyoungParkKeunhyoungLukeKimJuhanNam,
abstract = {Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image , text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolu-tional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. Our experiments show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million Song Dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features and show that they are sensitive to log-scaled frequency along layer, such as mel-frequency spectrogram that is widely used in music classification systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.01789v2},
author = {{Lee Jiyoung Park Keunhyoung Luke Kim Juhan Nam}, Jongpil},
eprint = {arXiv:1703.01789v2},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee Jiyoung Park Keunhyoung Luke Kim Juhan Nam - Unknown - SAMPLE-LEVEL DEEP CONVOLUTIONAL NEURAL NETWORKS FOR MUSIC AUTO-TAGGING USING.pdf:pdf},
mendeley-groups = {DeepLearning/Audio},
title = {{Sample-level deep convolutional neural networks for music auto-taggings using raw waveforms}},
url = {https://github.com/keunwoochoi/MSD{\_}split{\_}for{\_}}
}
@article{Hamel,
abstract = {This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset.},
author = {Hamel, Philippe and Lemieux, Simon and Bengio, Yoshua and Eck, Douglas},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamel et al. - Unknown - TEMPORAL POOLING AND MULTISCALE LEARNING FOR AUTOMATIC ANNOTATION AND RANKING OF MUSIC AUDIO.pdf:pdf},
mendeley-groups = {DeepLearning/Audio},
title = {{Temporal pooling and multiscale learning for automatic annotation and ranking of music audio}},
url = {http://www.iro.umontreal.ca/{~}lisa/bib/pub{\_}subject/finance/pointeurs/music{\_}pooling.pdf}
}
@inproceedings{Xu2017,
author = {Xu, Yong and Kong, Qiuqiang and Huang, Qiang and Wang, Wenwu and Plumbley, Mark D.},
booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2017.7966291},
file = {:C$\backslash$:/Users/miria/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2017 - Convolutional gated recurrent neural network incorporating spatial features for audio tagging.pdf:pdf},
isbn = {978-1-5090-6182-2},
mendeley-groups = {DeepLearning/Audio,DeepLearning},
month = {may},
pages = {3461--3466},
publisher = {IEEE},
title = {{Convolutional gated recurrent neural network incorporating spatial features for audio tagging}},
url = {http://ieeexplore.ieee.org/document/7966291/},
year = {2017}
}
