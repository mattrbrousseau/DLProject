{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set**\n",
    "\n",
    "The train set is meant to be for system development and includes **~9.5k samples unequally distributed among 41 categories**. The **minimum number of audio samples per category in the train set is 94, and the maximum 300**. The duration of the audio **samples ranges from 300ms to 30s** due to the diversity of the sound categories and the preferences of Freesound users when recording sounds.\n",
    "\n",
    "The train set is composed of **~3.7k manually-verified annotations and ~5.8k non-verified annotations**. **The quality of the non-verified annotations has been roughly estimated to be at least 65-70% in each sound category**. A flag for each annotation is provided which indicates whether or not that annotation has been manually verified. Participants can use this information during the development of their systems\n",
    "\n",
    "Maximum number of points for samples: 1323000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lib to work with music and audios https://librosa.github.io/librosa/\n",
    "import librosa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import constants\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "Piece of code copied from https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data\n",
    "'''\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "np.random.seed(1001)\n",
    "import os\n",
    "import shutil\n",
    "import IPython\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from scipy.io import wavfile\n",
    "#from tqdm import tqdm_notebook\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "#matplotlib.style.use('ggplot') - I prefer call the style as:\n",
    "plt.style.use(['seaborn-white','ggplot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(constants.basepath+\"database/train.csv\",header=0,names=['fname','label','verified'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some vizualiations of a sample</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fnamesample1 = constants.basepath+\"database/audio_train/00ad7068.wav\"\n",
    "rate1, data1 = wavfile.read(fnamesample1)\n",
    "\n",
    "fnamesample2 = constants.basepath+\"database/audio_train/00c9e799.wav\"\n",
    "rate2, data2 = wavfile.read(fnamesample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Sample 1: rate = \"+str(rate1)+\" data length = \"+str(len(data1)))\n",
    "print(\"Sample 2: rate = \"+str(rate2)+\" data length = \"+str(len(data2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2plot1 = pd.DataFrame(data1,columns = ['values'])\n",
    "data2plot1.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2plot2 = pd.DataFrame(data2,columns = ['values'])\n",
    "data2plot2.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ipd.Audio(fnamesample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ipd.Audio(fnamesample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Loading all wav files</b>\n",
    "\n",
    "1)We are gonna use only checked data for the first training and validations steps.\n",
    "\n",
    "2)Using wavfile.read on each files' name.\n",
    "\n",
    "3)Because all files has the same rate, 44.1 kHz, only the data as an numpy array is gonna be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkedDataset = dataset.loc[dataset.verified == 1,[\"fname\",\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepathWAVfiles = constants.basepath+\"database/audio_train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 36b3f8ec.wav do not exist!\n",
      "File 36ba581c.wav do not exist!\n",
      "File 36c81c76.wav do not exist!\n",
      "File 36ccf0c8.wav do not exist!\n",
      "File 36d06cc0.wav do not exist!\n",
      "File 36d20ab5.wav do not exist!\n",
      "File 36defe49.wav do not exist!\n",
      "File 37028a56.wav do not exist!\n",
      "File 3707da71.wav do not exist!\n",
      "File 37123461.wav do not exist!\n",
      "File 37349835.wav do not exist!\n",
      "File 375bfb8e.wav do not exist!\n",
      "File 3769aa70.wav do not exist!\n",
      "File 37b2166d.wav do not exist!\n",
      "File 37bffa64.wav do not exist!\n",
      "File 37cebbd4.wav do not exist!\n",
      "File 37d0eed3.wav do not exist!\n",
      "File 37d26e27.wav do not exist!\n",
      "File 37e5557d.wav do not exist!\n",
      "File 380ca85c.wav do not exist!\n"
     ]
    }
   ],
   "source": [
    "paddedData = []\n",
    "originalData = []\n",
    "#normalData = []\n",
    "target = []\n",
    "maxLength = 1323000\n",
    "for sample in checkedDataset.values:\n",
    "    fname,label = sample\n",
    "    try:\n",
    "        _,data = wavfile.read(basepathWAVfiles+fname)\n",
    "        paddedData.append(np.pad(data, (0,maxLength-len(data)), 'constant').astype(np.float))\n",
    "        originalData.append(data.astype(np.float))\n",
    "        #normalData.append(data/np.sum(data))\n",
    "        target.append(label)\n",
    "    except:\n",
    "        print(\"File \"+fname+' do not exist!')          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training and validation sets</b>\n",
    "\n",
    "We are gonna use only checked data for the first training and validations steps by using cross-validation of 10-fold, each time training on 90% of the data and testing on 10%.\n",
    "\n",
    "Features in paddedData #dimension [3710 samples, 1323000 features]\n",
    "\n",
    "Labels in target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sounds properties\n",
    "\n",
    "Source: https://www.nde-ed.org/EducationResources/HighSchool/Sound/components.htm\n",
    "\n",
    "Why are sounds different?\n",
    "\n",
    "The differences between sounds are caused by intensity, pitch, and tone.\n",
    "\n",
    "<b>Intensity</b>\n",
    "\n",
    "Sound is a wave and waves have amplitude, or height. Amplitude is a measure of energy. The more energy a wave has, the higher its amplitude. As amplitude increases, intensity also increases. Intensity is the amount of energy a sound has over an area. The same sound is more intense if you hear it in a smaller area. In general, we call sounds with a higher intensity louder. Intensity is measured in decibels.\n",
    "\n",
    "<b>Pitch</b>\n",
    "\n",
    "Pitch helps us distinguish between low and high sounds. Imagine that a singer sings the same note twice, one an octave above the other. \n",
    "\n",
    "Pitch depends on the frequency of a sound wave. Frequency is the number of wavelengths that fit into one unit of time. Frequencies are measured in hertz. One hertz is equal to one cycle of compression and rarefaction per second. High sounds have high frequencies and low sounds have low frequencies. \n",
    "\n",
    "<b>Tone & Harmonics</b>\n",
    "\n",
    "A violin sounds different than a flute playing the same pitch. This is because they have a different tone, or sound quality. When a source vibrates, it actually vibrates with many frequencies at the same time. Each of those frequencies produces a wave. Sound quality depends on the combination of different frequencies of sound waves.\n",
    "\n",
    "Imagine a guitar string tightly stretched. If we strum it, the energy from our finger is transferred to the string, causing it to vibrate. When the whole string vibrates, we hear the lowest pitch. This pitch is called the fundamental. Remember, the fundamental is really only one of many pitches that the string is producing. Parts of the string vibrating at frequencies higher than the fundamental are called overtones, while those vibrating in whole number multiples of the fundamental are called harmonics. A frequency of two times the fundamental will sound one octave higher and is called the second harmonic. A frequency four times the fundamental will sound two octaves higher and is called the fourth harmonic. Because the fundamental is one times itself, it is also called the first harmonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describing data in function of tone.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The Tonnetz is a pitch space defined by the network of relationships between\n",
    "musical pitches in just intonation. (source: http://www.nyu.edu/classes/bello/MIR_files/tonality.pdf)\n",
    "\n",
    "Package: https://librosa.github.io/librosa/generated/librosa.feature.tonnetz.html#librosa.feature.tonnetz\n",
    "'''\n",
    "\n",
    "tone=list(map(lambda x: librosa.feature.tonnetz(y=x, sr=44100, chroma=None), originalData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Could not execute the following line because it takes forever\n",
    "#tonePaddedData=list(map(lambda x: librosa.feature.tonnetz(y=x, sr=44100, chroma=None), paddedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes=[len(i[0]) for i in tone ]\n",
    "max2DTone = 2584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFJCAYAAAB3vj+vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAb6PxrgtzEll27TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x202b02a4550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizes = pd.DataFrame(sizes,columns=['size'])\n",
    "sizes.plot.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counter=collections.Counter(sizes)\n",
    "data2plot1 = pd.DataFrame.from_dict(counter, orient='index')\n",
    "data2plot1[\"size\"] = data2plot1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFXCAYAAABOYlxEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nggcAoAaR4AEAqEEkeAAAatD/B607PjGr2IdnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2029102b7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2plot1.plot.scatter(x=0, y='size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plots below, most of the samples (3311/3710 = 90%) have size smaller than 1000, that is why the array for each channel of the samples will be fixed to 1000.\n",
    "\n",
    "The code below, (1) cut the arrays in tone that are bigger than 1k or pad the arrays till the size 1000, (2) reshape the array to 1 dimension.\n",
    "\n",
    "The pad1000_1DToneOriginalData has each sample's tone limited to size 6000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad1000_1DToneOriginalData = []\n",
    "for d in tone:\n",
    "    if(len(d[0])>1000):\n",
    "        d=d[:,:1000]\n",
    "        d = np.reshape(d,[-1])\n",
    "    else:\n",
    "        newd=[]\n",
    "        j=0\n",
    "        for i in d:\n",
    "            i = np.pad(i,(0,1000-len(i)), 'constant')\n",
    "            newd.append(i)\n",
    "        d = np.reshape(newd,[-1])\n",
    "    pad1000_1DToneOriginalData.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizesPad1000=[len(i) for i in pad1000_1DToneOriginalData ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFVCAYAAAADqv1PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIAHAMBABDwAAAYi4AEAMBABDwCAgf4fkKvnh+zdAWkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20291af5780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizesPad1000 = pd.DataFrame(sizesPad1000,columns=['size'])\n",
    "sizesPad1000.plot.hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describing data in function of spectral centroid.</b>\n",
    "\n",
    "https://librosa.github.io/librosa/generated/librosa.feature.spectral_centroid.html#librosa.feature.spectral_centroid\n",
    "\n",
    "Function: librosa.feature.spectral_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specCentroid = list(map(lambda x: librosa.feature.spectral_centroid(y=x, sr=44100), originalData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sizesspecCentroid=[len(i[0]) for i in specCentroid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specCentroid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describing data in function of spectral bandwidth.</b>\n",
    "\n",
    "https://librosa.github.io/librosa/generated/librosa.feature.spectral_bandwidth.html#librosa.feature.spectral_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specBandwidth = list(map(lambda x: librosa.feature.spectral_bandwidth(y=x, sr=44100), originalData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sizesspecBandwidth=[len(i[0]) for i in specBandwidth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specBandwidth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Describing data in function of Mel-frequency cepstral coefficients (MFCCs).</b>\n",
    "\n",
    "https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html#librosa.feature.mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfcc = list(map(lambda x: librosa.feature.mfcc(y=x, sr=44100), originalData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MLP classifier</b>\n",
    "\n",
    "First results: Creating a standard MLP from scikit-learn, training and testing the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clfMLP = MLPClassifier(\n",
    "    hidden_layer_sizes = (2560,1024,256),\n",
    "    #The ith element represents the number of neurons in the ith hidden layer.\n",
    "    activation = 'relu',\n",
    "    #relu, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "    solver = 'adam',\n",
    "    #Optimizer. adam refers to a stochastic gradient-based optimizer. The default solver adam works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score.\n",
    "    #I need to use stochastic optimizer because I have too many samples (over 45 millions) and would be impraticable to have to execute all of them to then update the weights.\n",
    "    #alpha = \n",
    "    #Default 0.0001. L2 penalty (regularization term) parameter.\n",
    "    batch_size = 50,\n",
    "    #Size of minibatches for stochastic optimizers. When set to auto, batch_size=min(200, n_samples)\n",
    "    learning_rate = 'invscaling',\n",
    "    #Learning rate schedule for weight updates.\n",
    "\t#constant is a constant learning rate given by learning_rate_init.\n",
    "\t#invscaling gradually decreases the learning rate learning_rate_ at each time step t using an inverse scaling exponent of power_t. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "\t#adaptive keeps the learning rate constant to learning_rate_init as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if early_stopping is on, the current learning rate is divided by 5\n",
    "    learning_rate_init =0.1,\n",
    "    #Default 0.001. The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=sgd or adam.\n",
    "    #We attribute value to this using a grid search over values spaced approximately by factors of 3, i.e. {0,3; 0,1; 0,03; ... ; 0,0001}.\n",
    "    #max_iter = 2,\n",
    "    #Default 200. Maximum number of iterations. The solver iterates until convergence (determined by tol) or this number of iterations. For stochastic solvers (sgd, adam), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "    #shuffle : bool, optional, default True\n",
    "    #Whether to shuffle samples in each iteration. Only used when solver=sgd or adam.\n",
    "    #random_state =\n",
    "    #Default None. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "    tol = 1e-6,\n",
    "    #Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate is set to adaptive, convergence is considered to be reached and training stops.\n",
    "    #It was chosen based on the percentage that 1 sample wrong represents in the entire set of almost 46 million of samples: 1e-6\n",
    "    verbose = True,\n",
    "    #Whether to print progress messages to stdout.\n",
    "    #early_stopping = True,\n",
    "    #Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=sgd or adam\n",
    "    #validation_fraction =\n",
    "    #Default 0.1. The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    "    #beta_1 =\n",
    "    #Default 0.9. Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=adam\n",
    "    #beta_2 =\n",
    "    #Default 0.999. Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=adam\n",
    "    #epsilon =\n",
    "    #Default 1e-8. Value for numerical stability in adam. Only used when solver=adam\n",
    "    )\n",
    "\n",
    "def crossValidation(features,lables):\n",
    "\tprint('Scores for Scikit-Learn MLP:')\n",
    "\tscores = cross_val_score(clfMLP, features,lables,cv=10,scoring='accuracy')\n",
    "\tresultsMLP = [scores,scores.mean(),scores.std()]\n",
    "\tprint(str(resultsMLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Scikit-Learn MLP:\n",
      "Iteration 1, loss = 11.05276934\n",
      "Iteration 2, loss = 8.91301888\n",
      "Iteration 3, loss = 8.57031687\n",
      "Iteration 4, loss = 8.49690249\n",
      "Iteration 5, loss = 8.37421031\n",
      "Iteration 6, loss = 8.31786850\n",
      "Iteration 7, loss = 8.20376635\n",
      "Iteration 8, loss = 8.07987082\n",
      "Iteration 9, loss = 7.95361585\n",
      "Iteration 10, loss = 7.82309662\n",
      "Iteration 11, loss = 7.69851848\n",
      "Iteration 12, loss = 7.57268039\n",
      "Iteration 13, loss = 7.44756878\n",
      "Iteration 14, loss = 7.32973824\n",
      "Iteration 15, loss = 7.21008078\n",
      "Iteration 16, loss = 7.09114112\n",
      "Iteration 17, loss = 6.98520718\n",
      "Iteration 18, loss = 6.87062222\n",
      "Iteration 19, loss = 6.77265848\n",
      "Iteration 20, loss = 6.66373339\n",
      "Iteration 21, loss = 6.56283623\n",
      "Iteration 22, loss = 6.47889292\n",
      "Iteration 23, loss = 6.38041465\n",
      "Iteration 24, loss = 6.28679209\n",
      "Iteration 25, loss = 6.20397781\n",
      "Iteration 26, loss = 6.11424107\n",
      "Iteration 27, loss = 6.03282570\n",
      "Iteration 28, loss = 5.95070671\n",
      "Iteration 29, loss = 5.87546953\n",
      "Iteration 30, loss = 5.80115593\n",
      "Iteration 31, loss = 5.73222629\n",
      "Iteration 32, loss = 5.66415772\n",
      "Iteration 33, loss = 5.59254876\n",
      "Iteration 34, loss = 5.52646339\n",
      "Iteration 35, loss = 5.46533955\n",
      "Iteration 36, loss = 5.40867374\n",
      "Iteration 37, loss = 5.34958035\n",
      "Iteration 38, loss = 5.29229957\n",
      "Iteration 39, loss = 5.23529394\n",
      "Iteration 40, loss = 5.18259910\n",
      "Iteration 41, loss = 5.13600415\n",
      "Iteration 42, loss = 5.09117318\n",
      "Iteration 43, loss = 5.04998286\n",
      "Iteration 44, loss = 5.00459437\n",
      "Iteration 45, loss = 4.96118023\n",
      "Iteration 46, loss = 4.92148637\n",
      "Iteration 47, loss = 4.87782842\n",
      "Iteration 48, loss = 4.83953080\n",
      "Iteration 49, loss = 4.80285756\n",
      "Iteration 50, loss = 4.77093965\n",
      "Iteration 51, loss = 4.73532234\n",
      "Iteration 52, loss = 4.69973513\n",
      "Iteration 53, loss = 4.66276468\n",
      "Iteration 54, loss = 4.63523036\n",
      "Iteration 55, loss = 4.60559542\n",
      "Iteration 56, loss = 4.57704851\n",
      "Iteration 57, loss = 4.55991105\n",
      "Iteration 58, loss = 4.52626266\n",
      "Iteration 59, loss = 4.49874886\n",
      "Iteration 60, loss = 4.47088004\n",
      "Iteration 61, loss = 4.44608435\n",
      "Iteration 62, loss = 4.43051492\n",
      "Iteration 63, loss = 4.40678938\n",
      "Iteration 64, loss = 4.38610787\n",
      "Iteration 65, loss = 4.36507267\n",
      "Iteration 66, loss = 4.34371276\n",
      "Iteration 67, loss = 4.32681425\n",
      "Iteration 68, loss = 4.30743523\n",
      "Iteration 69, loss = 4.28645713\n",
      "Iteration 70, loss = 4.27388300\n",
      "Iteration 71, loss = 4.25534594\n",
      "Iteration 72, loss = 4.24376142\n",
      "Iteration 73, loss = 4.21961985\n",
      "Iteration 74, loss = 4.20598611\n",
      "Iteration 75, loss = 4.18999131\n",
      "Iteration 76, loss = 4.17898502\n",
      "Iteration 77, loss = 4.16572608\n",
      "Iteration 78, loss = 4.15355793\n",
      "Iteration 79, loss = 4.13948117\n",
      "Iteration 80, loss = 4.12891486\n",
      "Iteration 81, loss = 4.11719654\n",
      "Iteration 82, loss = 4.09850174\n",
      "Iteration 83, loss = 4.08606400\n",
      "Iteration 84, loss = 4.08049261\n",
      "Iteration 85, loss = 4.06415474\n",
      "Iteration 86, loss = 4.05878681\n",
      "Iteration 87, loss = 4.04870324\n",
      "Iteration 88, loss = 4.03306154\n",
      "Iteration 89, loss = 4.02365091\n",
      "Iteration 90, loss = 4.01714264\n",
      "Iteration 91, loss = 4.00252551\n",
      "Iteration 92, loss = 3.99709550\n",
      "Iteration 93, loss = 3.98594693\n",
      "Iteration 94, loss = 3.98040312\n",
      "Iteration 95, loss = 3.97035828\n",
      "Iteration 96, loss = 3.96229816\n",
      "Iteration 97, loss = 3.95089296\n",
      "Iteration 98, loss = 3.94091016\n",
      "Iteration 99, loss = 3.93300583\n",
      "Iteration 100, loss = 3.92975076\n",
      "Iteration 101, loss = 3.92095520\n",
      "Iteration 102, loss = 3.90719407\n",
      "Iteration 103, loss = 3.90477863\n",
      "Iteration 104, loss = 3.88864790\n",
      "Iteration 105, loss = 3.88672687\n",
      "Iteration 106, loss = 3.88318207\n",
      "Iteration 107, loss = 3.87315404\n",
      "Iteration 108, loss = 3.86484661\n",
      "Iteration 109, loss = 3.85559231\n",
      "Iteration 110, loss = 3.85332548\n",
      "Iteration 111, loss = 3.84376851\n",
      "Iteration 112, loss = 3.83865106\n",
      "Iteration 113, loss = 3.82955412\n",
      "Iteration 114, loss = 3.82680773\n",
      "Iteration 115, loss = 3.81911826\n",
      "Iteration 116, loss = 3.81666773\n",
      "Iteration 117, loss = 3.80607972\n",
      "Iteration 118, loss = 3.80471553\n",
      "Iteration 119, loss = 3.79753180\n",
      "Iteration 120, loss = 3.78811465\n",
      "Iteration 121, loss = 3.78883182\n",
      "Iteration 122, loss = 3.78311620\n",
      "Iteration 123, loss = 3.77722707\n",
      "Iteration 124, loss = 3.77156693\n",
      "Iteration 125, loss = 3.76669981\n",
      "Iteration 126, loss = 3.76880895\n",
      "Iteration 127, loss = 3.76060213\n",
      "Iteration 128, loss = 3.75728959\n",
      "Iteration 129, loss = 3.75251429\n",
      "Iteration 130, loss = 3.74400882\n",
      "Iteration 131, loss = 3.74331056\n",
      "Iteration 132, loss = 3.74188375\n",
      "Iteration 133, loss = 3.73530932\n",
      "Iteration 134, loss = 3.73418851\n",
      "Iteration 135, loss = 3.72961861\n",
      "Iteration 136, loss = 3.73018588\n",
      "Iteration 137, loss = 3.72682537\n",
      "Iteration 138, loss = 3.72087417\n",
      "Iteration 139, loss = 3.71750172\n",
      "Iteration 140, loss = 3.71770637\n",
      "Iteration 141, loss = 3.71429905\n",
      "Iteration 142, loss = 3.70659753\n",
      "Iteration 143, loss = 3.70865097\n",
      "Iteration 144, loss = 3.70308005\n",
      "Iteration 145, loss = 3.70698088\n",
      "Iteration 146, loss = 3.70469174\n",
      "Iteration 147, loss = 3.69559684\n",
      "Iteration 148, loss = 3.69925731\n",
      "Iteration 149, loss = 3.69535756\n",
      "Iteration 150, loss = 3.69246100\n",
      "Iteration 151, loss = 3.69019342\n",
      "Iteration 152, loss = 3.69071177\n",
      "Iteration 153, loss = 3.68929537\n",
      "Iteration 154, loss = 3.68365091\n",
      "Iteration 155, loss = 3.68698891\n",
      "Iteration 156, loss = 3.68462427\n",
      "Iteration 157, loss = 3.68593131\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.58815967\n",
      "Iteration 2, loss = 8.83042125\n",
      "Iteration 3, loss = 8.50429656\n",
      "Iteration 4, loss = 8.67584849\n",
      "Iteration 5, loss = 8.72176374\n",
      "Iteration 6, loss = 8.32945164\n",
      "Iteration 7, loss = 8.23333723\n",
      "Iteration 8, loss = 8.14000468\n",
      "Iteration 9, loss = 8.04345869\n",
      "Iteration 10, loss = 7.94839270\n",
      "Iteration 11, loss = 7.85764038\n",
      "Iteration 12, loss = 7.76267788\n",
      "Iteration 13, loss = 7.66958477\n",
      "Iteration 14, loss = 7.57874634\n",
      "Iteration 15, loss = 7.48740178\n",
      "Iteration 16, loss = 7.39735531\n",
      "Iteration 17, loss = 7.31016544\n",
      "Iteration 18, loss = 7.21955934\n",
      "Iteration 19, loss = 7.14118020\n",
      "Iteration 20, loss = 7.05442633\n",
      "Iteration 21, loss = 6.97493934\n",
      "Iteration 22, loss = 6.88803354\n",
      "Iteration 23, loss = 6.81268876\n",
      "Iteration 24, loss = 6.73359033\n",
      "Iteration 25, loss = 6.65310721\n",
      "Iteration 26, loss = 6.58261575\n",
      "Iteration 27, loss = 6.50587549\n",
      "Iteration 28, loss = 6.43688289\n",
      "Iteration 29, loss = 6.36404638\n",
      "Iteration 30, loss = 6.29520004\n",
      "Iteration 31, loss = 6.22621629\n",
      "Iteration 32, loss = 6.16065475\n",
      "Iteration 33, loss = 6.09551100\n",
      "Iteration 34, loss = 6.03512047\n",
      "Iteration 35, loss = 5.96742291\n",
      "Iteration 36, loss = 5.90660037\n",
      "Iteration 37, loss = 5.84297038\n",
      "Iteration 38, loss = 5.79361329\n",
      "Iteration 39, loss = 5.72949874\n",
      "Iteration 40, loss = 5.67662374\n",
      "Iteration 41, loss = 5.62525928\n",
      "Iteration 42, loss = 5.57060139\n",
      "Iteration 43, loss = 5.51866269\n",
      "Iteration 44, loss = 5.46851223\n",
      "Iteration 45, loss = 5.41646405\n",
      "Iteration 46, loss = 5.36960943\n",
      "Iteration 47, loss = 5.32244973\n",
      "Iteration 48, loss = 5.27741914\n",
      "Iteration 49, loss = 5.23260140\n",
      "Iteration 50, loss = 5.18949152\n",
      "Iteration 51, loss = 5.14204098\n",
      "Iteration 52, loss = 5.10264162\n",
      "Iteration 53, loss = 5.06215781\n",
      "Iteration 54, loss = 5.02040427\n",
      "Iteration 55, loss = 4.98456646\n",
      "Iteration 56, loss = 4.94469388\n",
      "Iteration 57, loss = 4.90751240\n",
      "Iteration 58, loss = 4.87703247\n",
      "Iteration 59, loss = 4.83621870\n",
      "Iteration 60, loss = 4.80556270\n",
      "Iteration 61, loss = 4.77154781\n",
      "Iteration 62, loss = 4.73866826\n",
      "Iteration 63, loss = 4.70977061\n",
      "Iteration 64, loss = 4.67536882\n",
      "Iteration 65, loss = 4.64709519\n",
      "Iteration 66, loss = 4.61986065\n",
      "Iteration 67, loss = 4.59137857\n",
      "Iteration 68, loss = 4.56485303\n",
      "Iteration 69, loss = 4.53803860\n",
      "Iteration 70, loss = 4.51323009\n",
      "Iteration 71, loss = 4.48702302\n",
      "Iteration 72, loss = 4.46446788\n",
      "Iteration 73, loss = 4.44506826\n",
      "Iteration 74, loss = 4.41808039\n",
      "Iteration 75, loss = 4.39621802\n",
      "Iteration 76, loss = 4.37327696\n",
      "Iteration 77, loss = 4.35495594\n",
      "Iteration 78, loss = 4.33593428\n",
      "Iteration 79, loss = 4.31188161\n",
      "Iteration 80, loss = 4.29514988\n",
      "Iteration 81, loss = 4.27743429\n",
      "Iteration 82, loss = 4.25795905\n",
      "Iteration 83, loss = 4.24504735\n",
      "Iteration 84, loss = 4.22308083\n",
      "Iteration 85, loss = 4.20988921\n",
      "Iteration 86, loss = 4.19214051\n",
      "Iteration 87, loss = 4.17300614\n",
      "Iteration 88, loss = 4.15821951\n",
      "Iteration 89, loss = 4.14882094\n",
      "Iteration 90, loss = 4.13033661\n",
      "Iteration 91, loss = 4.11894974\n",
      "Iteration 92, loss = 4.10466525\n",
      "Iteration 93, loss = 4.09169800\n",
      "Iteration 94, loss = 4.07805107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, loss = 4.06413743\n",
      "Iteration 96, loss = 4.05046855\n",
      "Iteration 97, loss = 4.04111150\n",
      "Iteration 98, loss = 4.02898296\n",
      "Iteration 99, loss = 4.01797409\n",
      "Iteration 100, loss = 4.00791844\n",
      "Iteration 101, loss = 3.99236533\n",
      "Iteration 102, loss = 3.98547753\n",
      "Iteration 103, loss = 3.97835792\n",
      "Iteration 104, loss = 3.96600945\n",
      "Iteration 105, loss = 3.95231522\n",
      "Iteration 106, loss = 3.94589744\n",
      "Iteration 107, loss = 3.93728433\n",
      "Iteration 108, loss = 3.92636148\n",
      "Iteration 109, loss = 3.91861046\n",
      "Iteration 110, loss = 3.91021535\n",
      "Iteration 111, loss = 3.89856559\n",
      "Iteration 112, loss = 3.89231607\n",
      "Iteration 113, loss = 3.88510808\n",
      "Iteration 114, loss = 3.88058186\n",
      "Iteration 115, loss = 3.86725392\n",
      "Iteration 116, loss = 3.86609788\n",
      "Iteration 117, loss = 3.85690238\n",
      "Iteration 118, loss = 3.84649797\n",
      "Iteration 119, loss = 3.84395728\n",
      "Iteration 120, loss = 3.83560886\n",
      "Iteration 121, loss = 3.82810757\n",
      "Iteration 122, loss = 3.82259148\n",
      "Iteration 123, loss = 3.81633030\n",
      "Iteration 124, loss = 3.81241757\n",
      "Iteration 125, loss = 3.80760688\n",
      "Iteration 126, loss = 3.80033262\n",
      "Iteration 127, loss = 3.79608114\n",
      "Iteration 128, loss = 3.78800460\n",
      "Iteration 129, loss = 3.78886263\n",
      "Iteration 130, loss = 3.78183515\n",
      "Iteration 131, loss = 3.77149137\n",
      "Iteration 132, loss = 3.76715051\n",
      "Iteration 133, loss = 3.76685678\n",
      "Iteration 134, loss = 3.76742865\n",
      "Iteration 135, loss = 3.75929325\n",
      "Iteration 136, loss = 3.75458634\n",
      "Iteration 137, loss = 3.75131855\n",
      "Iteration 138, loss = 3.74679453\n",
      "Iteration 139, loss = 3.73950687\n",
      "Iteration 140, loss = 3.73367060\n",
      "Iteration 141, loss = 3.73658685\n",
      "Iteration 142, loss = 3.72559385\n",
      "Iteration 143, loss = 3.72481870\n",
      "Iteration 144, loss = 3.72546584\n",
      "Iteration 145, loss = 3.72325602\n",
      "Iteration 146, loss = 3.71460816\n",
      "Iteration 147, loss = 3.71119832\n",
      "Iteration 148, loss = 3.71249834\n",
      "Iteration 149, loss = 3.71037523\n",
      "Iteration 150, loss = 3.70596602\n",
      "Iteration 151, loss = 3.70237708\n",
      "Iteration 152, loss = 3.69894250\n",
      "Iteration 153, loss = 3.70194965\n",
      "Iteration 154, loss = 3.70158530\n",
      "Iteration 155, loss = 3.69916724\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.73858773\n",
      "Iteration 2, loss = 8.66399343\n",
      "Iteration 3, loss = 8.50126405\n",
      "Iteration 4, loss = 8.36536841\n",
      "Iteration 5, loss = 8.23146654\n",
      "Iteration 6, loss = 8.09438115\n",
      "Iteration 7, loss = 7.95537488\n",
      "Iteration 8, loss = 7.81866587\n",
      "Iteration 9, loss = 7.68696392\n",
      "Iteration 10, loss = 7.55115938\n",
      "Iteration 11, loss = 7.42288361\n",
      "Iteration 12, loss = 7.29919804\n",
      "Iteration 13, loss = 7.17316009\n",
      "Iteration 14, loss = 7.05490459\n",
      "Iteration 15, loss = 6.93845498\n",
      "Iteration 16, loss = 6.82193718\n",
      "Iteration 17, loss = 6.71390139\n",
      "Iteration 18, loss = 6.60911293\n",
      "Iteration 19, loss = 6.51129282\n",
      "Iteration 20, loss = 6.40813117\n",
      "Iteration 21, loss = 6.31749848\n",
      "Iteration 22, loss = 6.22222644\n",
      "Iteration 23, loss = 6.13351403\n",
      "Iteration 24, loss = 6.05012805\n",
      "Iteration 25, loss = 5.96606128\n",
      "Iteration 26, loss = 5.89184422\n",
      "Iteration 27, loss = 5.80883940\n",
      "Iteration 28, loss = 5.73889518\n",
      "Iteration 29, loss = 5.66542785\n",
      "Iteration 30, loss = 5.59806734\n",
      "Iteration 31, loss = 5.53606886\n",
      "Iteration 32, loss = 5.46940304\n",
      "Iteration 33, loss = 5.41196065\n",
      "Iteration 34, loss = 5.35275317\n",
      "Iteration 35, loss = 5.29323728\n",
      "Iteration 36, loss = 5.24596639\n",
      "Iteration 37, loss = 5.19340638\n",
      "Iteration 38, loss = 5.14265820\n",
      "Iteration 39, loss = 5.09685022\n",
      "Iteration 40, loss = 5.04714069\n",
      "Iteration 41, loss = 5.00527850\n",
      "Iteration 42, loss = 4.95855138\n",
      "Iteration 43, loss = 4.91587951\n",
      "Iteration 44, loss = 4.88133207\n",
      "Iteration 45, loss = 4.84487077\n",
      "Iteration 46, loss = 4.80548255\n",
      "Iteration 47, loss = 4.76934688\n",
      "Iteration 48, loss = 4.73483864\n",
      "Iteration 49, loss = 4.70228679\n",
      "Iteration 50, loss = 4.66927555\n",
      "Iteration 51, loss = 4.64055891\n",
      "Iteration 52, loss = 4.60884940\n",
      "Iteration 53, loss = 4.58320403\n",
      "Iteration 54, loss = 4.55681452\n",
      "Iteration 55, loss = 4.52709455\n",
      "Iteration 56, loss = 4.50428577\n",
      "Iteration 57, loss = 4.48004794\n",
      "Iteration 58, loss = 4.45919469\n",
      "Iteration 59, loss = 4.43053588\n",
      "Iteration 60, loss = 4.41003158\n",
      "Iteration 61, loss = 4.38796307\n",
      "Iteration 62, loss = 4.36874755\n",
      "Iteration 63, loss = 4.34852623\n",
      "Iteration 64, loss = 4.33169519\n",
      "Iteration 65, loss = 4.31194488\n",
      "Iteration 66, loss = 4.29084347\n",
      "Iteration 67, loss = 4.27830731\n",
      "Iteration 68, loss = 4.25416197\n",
      "Iteration 69, loss = 10.28887877\n",
      "Iteration 70, loss = 14.93121708\n",
      "Iteration 71, loss = 12.06789670\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.41210396\n",
      "Iteration 2, loss = 8.85875826\n",
      "Iteration 3, loss = 8.83318322\n",
      "Iteration 4, loss = 8.54689444\n",
      "Iteration 5, loss = 8.37822077\n",
      "Iteration 6, loss = 8.20561483\n",
      "Iteration 7, loss = 8.03553557\n",
      "Iteration 8, loss = 7.87255776\n",
      "Iteration 9, loss = 7.71460071\n",
      "Iteration 10, loss = 7.55775651\n",
      "Iteration 11, loss = 7.40938183\n",
      "Iteration 12, loss = 7.26514731\n",
      "Iteration 13, loss = 7.13074695\n",
      "Iteration 14, loss = 6.99981631\n",
      "Iteration 15, loss = 6.87302058\n",
      "Iteration 16, loss = 6.75273772\n",
      "Iteration 17, loss = 6.63526210\n",
      "Iteration 18, loss = 6.51907799\n",
      "Iteration 19, loss = 6.42001919\n",
      "Iteration 20, loss = 6.31545953\n",
      "Iteration 21, loss = 6.21729314\n",
      "Iteration 22, loss = 6.12647916\n",
      "Iteration 23, loss = 6.02996959\n",
      "Iteration 24, loss = 5.94648429\n",
      "Iteration 25, loss = 5.86144288\n",
      "Iteration 26, loss = 5.78580294\n",
      "Iteration 27, loss = 5.70705997\n",
      "Iteration 28, loss = 5.63205030\n",
      "Iteration 29, loss = 5.61749565\n",
      "Iteration 30, loss = 9.43773374\n",
      "Iteration 31, loss = 9.80202619\n",
      "Iteration 32, loss = 9.32276876\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.18386868\n",
      "Iteration 2, loss = 8.64448419\n",
      "Iteration 3, loss = 8.50083004\n",
      "Iteration 4, loss = 8.40214710\n",
      "Iteration 5, loss = 8.29988259\n",
      "Iteration 6, loss = 8.19780516\n",
      "Iteration 7, loss = 8.09222133\n",
      "Iteration 8, loss = 7.98365290\n",
      "Iteration 9, loss = 7.87977564\n",
      "Iteration 10, loss = 7.76265031\n",
      "Iteration 11, loss = 7.65654620\n",
      "Iteration 12, loss = 7.54553330\n",
      "Iteration 13, loss = 7.43611060\n",
      "Iteration 14, loss = 7.32911903\n",
      "Iteration 15, loss = 7.22314658\n",
      "Iteration 16, loss = 7.11820241\n",
      "Iteration 17, loss = 7.02027288\n",
      "Iteration 18, loss = 6.90542039\n",
      "Iteration 19, loss = 6.81207337\n",
      "Iteration 20, loss = 6.71490255\n",
      "Iteration 21, loss = 6.61770626\n",
      "Iteration 22, loss = 6.52473975\n",
      "Iteration 23, loss = 6.43179549\n",
      "Iteration 24, loss = 6.34570106\n",
      "Iteration 25, loss = 6.25844083\n",
      "Iteration 26, loss = 6.18137220\n",
      "Iteration 27, loss = 6.09911752\n",
      "Iteration 28, loss = 6.01874586\n",
      "Iteration 29, loss = 5.93995303\n",
      "Iteration 30, loss = 5.86027100\n",
      "Iteration 31, loss = 5.79090572\n",
      "Iteration 32, loss = 5.72339927\n",
      "Iteration 33, loss = 5.65356465\n",
      "Iteration 34, loss = 5.58318027\n",
      "Iteration 35, loss = 5.52045499\n",
      "Iteration 36, loss = 5.46215686\n",
      "Iteration 37, loss = 5.40317600\n",
      "Iteration 38, loss = 5.34240203\n",
      "Iteration 39, loss = 5.29213170\n",
      "Iteration 40, loss = 5.23313493\n",
      "Iteration 41, loss = 5.18254220\n",
      "Iteration 42, loss = 5.13197465\n",
      "Iteration 43, loss = 5.08058045\n",
      "Iteration 44, loss = 5.03097676\n",
      "Iteration 45, loss = 4.99166119\n",
      "Iteration 46, loss = 4.94382217\n",
      "Iteration 47, loss = 4.90301182\n",
      "Iteration 48, loss = 4.86199854\n",
      "Iteration 49, loss = 4.82886425\n",
      "Iteration 50, loss = 4.78144693\n",
      "Iteration 51, loss = 4.74429533\n",
      "Iteration 52, loss = 4.71393265\n",
      "Iteration 53, loss = 4.67467431\n",
      "Iteration 54, loss = 4.64055775\n",
      "Iteration 55, loss = 4.61469693\n",
      "Iteration 56, loss = 4.58494268\n",
      "Iteration 57, loss = 4.55252390\n",
      "Iteration 58, loss = 4.52351673\n",
      "Iteration 59, loss = 4.49701326\n",
      "Iteration 60, loss = 4.47369188\n",
      "Iteration 61, loss = 4.44408212\n",
      "Iteration 62, loss = 4.41787301\n",
      "Iteration 63, loss = 4.39338011\n",
      "Iteration 64, loss = 4.36969786\n",
      "Iteration 65, loss = 4.34920594\n",
      "Iteration 66, loss = 4.33373237\n",
      "Iteration 67, loss = 4.31052603\n",
      "Iteration 68, loss = 4.28868402\n",
      "Iteration 69, loss = 4.26657979\n",
      "Iteration 70, loss = 4.25029119\n",
      "Iteration 71, loss = 4.23112023\n",
      "Iteration 72, loss = 4.21455185\n",
      "Iteration 73, loss = 4.19418270\n",
      "Iteration 74, loss = 4.18368629\n",
      "Iteration 75, loss = 4.16661057\n",
      "Iteration 76, loss = 4.15086404\n",
      "Iteration 77, loss = 4.12939235\n",
      "Iteration 78, loss = 4.12153955\n",
      "Iteration 79, loss = 4.10761268\n",
      "Iteration 80, loss = 4.09626155\n",
      "Iteration 81, loss = 4.07811116\n",
      "Iteration 82, loss = 4.06493770\n",
      "Iteration 83, loss = 4.05139196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 4.04062894\n",
      "Iteration 85, loss = 4.03109584\n",
      "Iteration 86, loss = 4.01763219\n",
      "Iteration 87, loss = 4.00179556\n",
      "Iteration 88, loss = 3.99580080\n",
      "Iteration 89, loss = 3.98613801\n",
      "Iteration 90, loss = 3.97696528\n",
      "Iteration 91, loss = 3.96377026\n",
      "Iteration 92, loss = 3.95492153\n",
      "Iteration 93, loss = 3.94436185\n",
      "Iteration 94, loss = 3.93955268\n",
      "Iteration 95, loss = 3.93075988\n",
      "Iteration 96, loss = 3.92679711\n",
      "Iteration 97, loss = 3.91324105\n",
      "Iteration 98, loss = 3.90321648\n",
      "Iteration 99, loss = 3.89732087\n",
      "Iteration 100, loss = 3.88341669\n",
      "Iteration 101, loss = 3.87608860\n",
      "Iteration 102, loss = 3.86897130\n",
      "Iteration 103, loss = 3.86577029\n",
      "Iteration 104, loss = 3.85871159\n",
      "Iteration 105, loss = 3.85303960\n",
      "Iteration 106, loss = 3.84132064\n",
      "Iteration 107, loss = 3.83152331\n",
      "Iteration 108, loss = 3.83440968\n",
      "Iteration 109, loss = 3.82363049\n",
      "Iteration 110, loss = 3.81401406\n",
      "Iteration 111, loss = 3.80986105\n",
      "Iteration 112, loss = 3.80741084\n",
      "Iteration 113, loss = 3.79774300\n",
      "Iteration 114, loss = 3.79352710\n",
      "Iteration 115, loss = 3.79454445\n",
      "Iteration 116, loss = 3.78514086\n",
      "Iteration 117, loss = 3.77673065\n",
      "Iteration 118, loss = 3.77471822\n",
      "Iteration 119, loss = 3.76995317\n",
      "Iteration 120, loss = 3.76771422\n",
      "Iteration 121, loss = 3.75993839\n",
      "Iteration 122, loss = 3.75885605\n",
      "Iteration 123, loss = 3.75105753\n",
      "Iteration 124, loss = 3.75146957\n",
      "Iteration 125, loss = 3.74169226\n",
      "Iteration 126, loss = 3.74305303\n",
      "Iteration 127, loss = 3.73394006\n",
      "Iteration 128, loss = 3.73331142\n",
      "Iteration 129, loss = 3.73112857\n",
      "Iteration 130, loss = 3.72345247\n",
      "Iteration 131, loss = 3.72462611\n",
      "Iteration 132, loss = 3.72215402\n",
      "Iteration 133, loss = 3.71610172\n",
      "Iteration 134, loss = 3.71383887\n",
      "Iteration 135, loss = 3.71253033\n",
      "Iteration 136, loss = 3.70900652\n",
      "Iteration 137, loss = 3.70494823\n",
      "Iteration 138, loss = 3.70188218\n",
      "Iteration 139, loss = 3.70308169\n",
      "Iteration 140, loss = 3.69949662\n",
      "Iteration 141, loss = 3.69752159\n",
      "Iteration 142, loss = 3.69420262\n",
      "Iteration 143, loss = 3.68674911\n",
      "Iteration 144, loss = 3.69076094\n",
      "Iteration 145, loss = 3.68937666\n",
      "Iteration 146, loss = 3.68845954\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.31763068\n",
      "Iteration 2, loss = 8.90924546\n",
      "Iteration 3, loss = 8.81081822\n",
      "Iteration 4, loss = 8.70180599\n",
      "Iteration 5, loss = 8.52898383\n",
      "Iteration 6, loss = 8.36249239\n",
      "Iteration 7, loss = 8.19799766\n",
      "Iteration 8, loss = 8.03176695\n",
      "Iteration 9, loss = 7.86528362\n",
      "Iteration 10, loss = 7.70670863\n",
      "Iteration 11, loss = 7.54678310\n",
      "Iteration 12, loss = 7.39547759\n",
      "Iteration 13, loss = 7.24611285\n",
      "Iteration 14, loss = 7.10022999\n",
      "Iteration 15, loss = 6.96027282\n",
      "Iteration 16, loss = 6.83238571\n",
      "Iteration 17, loss = 6.70611447\n",
      "Iteration 18, loss = 6.57953806\n",
      "Iteration 19, loss = 6.45432084\n",
      "Iteration 20, loss = 6.34462333\n",
      "Iteration 21, loss = 6.23060907\n",
      "Iteration 22, loss = 6.12670656\n",
      "Iteration 23, loss = 6.02896548\n",
      "Iteration 24, loss = 5.92719390\n",
      "Iteration 25, loss = 5.83350401\n",
      "Iteration 26, loss = 5.74443487\n",
      "Iteration 27, loss = 5.66241353\n",
      "Iteration 28, loss = 5.58000418\n",
      "Iteration 29, loss = 5.50133135\n",
      "Iteration 30, loss = 5.42453290\n",
      "Iteration 31, loss = 5.35720401\n",
      "Iteration 32, loss = 5.28766994\n",
      "Iteration 33, loss = 5.21662423\n",
      "Iteration 34, loss = 5.15417691\n",
      "Iteration 35, loss = 5.09516500\n",
      "Iteration 36, loss = 5.03890495\n",
      "Iteration 37, loss = 4.98967167\n",
      "Iteration 38, loss = 4.93022507\n",
      "Iteration 39, loss = 4.88276016\n",
      "Iteration 40, loss = 4.83159795\n",
      "Iteration 41, loss = 4.78308592\n",
      "Iteration 42, loss = 4.73633699\n",
      "Iteration 43, loss = 4.69725987\n",
      "Iteration 44, loss = 4.65237098\n",
      "Iteration 45, loss = 4.61881426\n",
      "Iteration 46, loss = 4.57597171\n",
      "Iteration 47, loss = 4.54244475\n",
      "Iteration 48, loss = 4.51422061\n",
      "Iteration 49, loss = 4.47545953\n",
      "Iteration 50, loss = 4.44834640\n",
      "Iteration 51, loss = 4.42240387\n",
      "Iteration 52, loss = 4.39238267\n",
      "Iteration 53, loss = 4.35928272\n",
      "Iteration 54, loss = 4.33678091\n",
      "Iteration 55, loss = 4.31139956\n",
      "Iteration 56, loss = 4.28184432\n",
      "Iteration 57, loss = 4.26422851\n",
      "Iteration 58, loss = 4.23860528\n",
      "Iteration 59, loss = 4.21803588\n",
      "Iteration 60, loss = 4.20291796\n",
      "Iteration 61, loss = 4.17861537\n",
      "Iteration 62, loss = 4.16017497\n",
      "Iteration 63, loss = 4.14216807\n",
      "Iteration 64, loss = 4.12813629\n",
      "Iteration 65, loss = 4.11032619\n",
      "Iteration 66, loss = 4.09074700\n",
      "Iteration 67, loss = 4.07832163\n",
      "Iteration 68, loss = 4.06101138\n",
      "Iteration 69, loss = 4.04979957\n",
      "Iteration 70, loss = 4.03110237\n",
      "Iteration 71, loss = 4.02414554\n",
      "Iteration 72, loss = 4.00940908\n",
      "Iteration 73, loss = 3.99343472\n",
      "Iteration 74, loss = 3.98867475\n",
      "Iteration 75, loss = 3.97337409\n",
      "Iteration 76, loss = 3.96205998\n",
      "Iteration 77, loss = 3.94714189\n",
      "Iteration 78, loss = 3.94055446\n",
      "Iteration 79, loss = 3.93344518\n",
      "Iteration 80, loss = 3.92325496\n",
      "Iteration 81, loss = 3.90738902\n",
      "Iteration 82, loss = 3.90096865\n",
      "Iteration 83, loss = 3.89586868\n",
      "Iteration 84, loss = 3.88547437\n",
      "Iteration 85, loss = 3.87991012\n",
      "Iteration 86, loss = 3.87581753\n",
      "Iteration 87, loss = 3.85936204\n",
      "Iteration 88, loss = 3.85519654\n",
      "Iteration 89, loss = 3.84741382\n",
      "Iteration 90, loss = 3.84055734\n",
      "Iteration 91, loss = 3.83655188\n",
      "Iteration 92, loss = 3.82493970\n",
      "Iteration 93, loss = 3.82130387\n",
      "Iteration 94, loss = 3.81275955\n",
      "Iteration 95, loss = 3.80957916\n",
      "Iteration 96, loss = 3.80228042\n",
      "Iteration 97, loss = 3.79947812\n",
      "Iteration 98, loss = 3.79537484\n",
      "Iteration 99, loss = 3.78602643\n",
      "Iteration 100, loss = 3.77850825\n",
      "Iteration 101, loss = 3.78262176\n",
      "Iteration 102, loss = 3.77374074\n",
      "Iteration 103, loss = 3.77101306\n",
      "Iteration 104, loss = 3.76156220\n",
      "Iteration 105, loss = 3.76065992\n",
      "Iteration 106, loss = 3.75171757\n",
      "Iteration 107, loss = 3.74917548\n",
      "Iteration 108, loss = 3.74895192\n",
      "Iteration 109, loss = 3.74045474\n",
      "Iteration 110, loss = 3.73863010\n",
      "Iteration 111, loss = 3.73624779\n",
      "Iteration 112, loss = 3.73308984\n",
      "Iteration 113, loss = 3.72637526\n",
      "Iteration 114, loss = 3.72324011\n",
      "Iteration 115, loss = 3.72319430\n",
      "Iteration 116, loss = 3.72010834\n",
      "Iteration 117, loss = 3.71711514\n",
      "Iteration 118, loss = 3.71095256\n",
      "Iteration 119, loss = 3.70893730\n",
      "Iteration 120, loss = 3.70688734\n",
      "Iteration 121, loss = 3.70532029\n",
      "Iteration 122, loss = 3.70461092\n",
      "Iteration 123, loss = 3.69980180\n",
      "Iteration 124, loss = 3.70291375\n",
      "Iteration 125, loss = 3.69444684\n",
      "Iteration 126, loss = 3.69366546\n",
      "Iteration 127, loss = 3.69262085\n",
      "Iteration 128, loss = 3.69264139\n",
      "Iteration 129, loss = 3.69157820\n",
      "Iteration 130, loss = 3.68688221\n",
      "Iteration 131, loss = 3.68848363\n",
      "Iteration 132, loss = 3.68318496\n",
      "Iteration 133, loss = 3.68408524\n",
      "Iteration 134, loss = 3.68587145\n",
      "Iteration 135, loss = 3.68126919\n",
      "Iteration 136, loss = 3.68342740\n",
      "Iteration 137, loss = 3.67665094\n",
      "Iteration 138, loss = 3.67758076\n",
      "Iteration 139, loss = 3.67515958\n",
      "Iteration 140, loss = 3.67664513\n",
      "Iteration 141, loss = 3.67658812\n",
      "Iteration 142, loss = 3.67657286\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.43635330\n",
      "Iteration 2, loss = 8.73134012\n",
      "Iteration 3, loss = 8.55618880\n",
      "Iteration 4, loss = 8.39642844\n",
      "Iteration 5, loss = 8.23362277\n",
      "Iteration 6, loss = 8.06837373\n",
      "Iteration 7, loss = 7.90353621\n",
      "Iteration 8, loss = 7.74275521\n",
      "Iteration 9, loss = 7.57847686\n",
      "Iteration 10, loss = 7.41990307\n",
      "Iteration 11, loss = 7.26872139\n",
      "Iteration 12, loss = 7.11203845\n",
      "Iteration 13, loss = 6.97516835\n",
      "Iteration 14, loss = 6.83487947\n",
      "Iteration 15, loss = 6.69661464\n",
      "Iteration 16, loss = 6.57485202\n",
      "Iteration 17, loss = 6.44662041\n",
      "Iteration 18, loss = 6.32960679\n",
      "Iteration 19, loss = 6.21162250\n",
      "Iteration 20, loss = 6.10471706\n",
      "Iteration 21, loss = 6.00661985\n",
      "Iteration 22, loss = 5.90032210\n",
      "Iteration 23, loss = 5.81078804\n",
      "Iteration 24, loss = 5.72251673\n",
      "Iteration 25, loss = 5.63650599\n",
      "Iteration 26, loss = 5.55612104\n",
      "Iteration 27, loss = 5.47751795\n",
      "Iteration 28, loss = 5.40513449\n",
      "Iteration 29, loss = 5.32947696\n",
      "Iteration 30, loss = 5.26061543\n",
      "Iteration 31, loss = 5.20381627\n",
      "Iteration 32, loss = 5.13595412\n",
      "Iteration 33, loss = 5.07642799\n",
      "Iteration 34, loss = 5.02263948\n",
      "Iteration 35, loss = 4.96837630\n",
      "Iteration 36, loss = 4.91841838\n",
      "Iteration 37, loss = 4.86973666\n",
      "Iteration 38, loss = 4.82082658\n",
      "Iteration 39, loss = 4.77929908\n",
      "Iteration 40, loss = 4.73663901\n",
      "Iteration 41, loss = 4.69797702\n",
      "Iteration 42, loss = 4.65767182\n",
      "Iteration 43, loss = 4.62206697\n",
      "Iteration 44, loss = 4.58132034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 4.56001777\n",
      "Iteration 46, loss = 4.51635680\n",
      "Iteration 47, loss = 4.49081265\n",
      "Iteration 48, loss = 4.46242892\n",
      "Iteration 49, loss = 4.43483914\n",
      "Iteration 50, loss = 4.40484565\n",
      "Iteration 51, loss = 4.38180083\n",
      "Iteration 52, loss = 4.35712052\n",
      "Iteration 53, loss = 4.33309090\n",
      "Iteration 54, loss = 4.31165297\n",
      "Iteration 55, loss = 4.29128400\n",
      "Iteration 56, loss = 12.85706045\n",
      "Iteration 57, loss = 19.88927797\n",
      "Iteration 58, loss = 15.49837130\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.19892440\n",
      "Iteration 2, loss = 8.36035705\n",
      "Iteration 3, loss = 8.56215615\n",
      "Iteration 4, loss = 8.30624705\n",
      "Iteration 5, loss = 8.19143431\n",
      "Iteration 6, loss = 8.07770876\n",
      "Iteration 7, loss = 7.94971220\n",
      "Iteration 8, loss = 7.83369649\n",
      "Iteration 9, loss = 7.70795640\n",
      "Iteration 10, loss = 7.58183609\n",
      "Iteration 11, loss = 7.45815406\n",
      "Iteration 12, loss = 7.33728619\n",
      "Iteration 13, loss = 7.21669463\n",
      "Iteration 14, loss = 7.10122400\n",
      "Iteration 15, loss = 6.97798600\n",
      "Iteration 16, loss = 6.86602302\n",
      "Iteration 17, loss = 6.75456048\n",
      "Iteration 18, loss = 6.64258608\n",
      "Iteration 19, loss = 6.53779767\n",
      "Iteration 20, loss = 6.43037441\n",
      "Iteration 21, loss = 6.32917929\n",
      "Iteration 22, loss = 6.23032551\n",
      "Iteration 23, loss = 6.13579552\n",
      "Iteration 24, loss = 6.04355572\n",
      "Iteration 25, loss = 5.95708244\n",
      "Iteration 26, loss = 5.86955711\n",
      "Iteration 27, loss = 5.78994259\n",
      "Iteration 28, loss = 5.71065245\n",
      "Iteration 29, loss = 5.63208675\n",
      "Iteration 30, loss = 5.56094007\n",
      "Iteration 31, loss = 5.48438658\n",
      "Iteration 32, loss = 5.41775332\n",
      "Iteration 33, loss = 5.34852258\n",
      "Iteration 34, loss = 5.29046689\n",
      "Iteration 35, loss = 9.93602602\n",
      "Iteration 36, loss = 11.27595341\n",
      "Iteration 37, loss = 10.42331997\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 11.37472090\n",
      "Iteration 2, loss = 8.66617774\n",
      "Iteration 3, loss = 8.50896730\n",
      "Iteration 4, loss = 8.36524680\n",
      "Iteration 5, loss = 8.22109263\n",
      "Iteration 6, loss = 8.07463703\n",
      "Iteration 7, loss = 7.92832603\n",
      "Iteration 8, loss = 7.78132653\n",
      "Iteration 9, loss = 7.63402479\n",
      "Iteration 10, loss = 7.48899246\n",
      "Iteration 11, loss = 7.34924065\n",
      "Iteration 12, loss = 7.21255558\n",
      "Iteration 13, loss = 7.08117461\n",
      "Iteration 14, loss = 6.95558951\n",
      "Iteration 15, loss = 6.82839645\n",
      "Iteration 16, loss = 6.71617643\n",
      "Iteration 17, loss = 6.59950509\n",
      "Iteration 18, loss = 6.49360476\n",
      "Iteration 19, loss = 6.39088869\n",
      "Iteration 20, loss = 6.29141056\n",
      "Iteration 21, loss = 6.18854350\n",
      "Iteration 22, loss = 6.10304043\n",
      "Iteration 23, loss = 6.01058265\n",
      "Iteration 24, loss = 5.93192056\n",
      "Iteration 25, loss = 5.84728862\n",
      "Iteration 26, loss = 5.77345209\n",
      "Iteration 27, loss = 5.69563098\n",
      "Iteration 28, loss = 5.62472046\n",
      "Iteration 29, loss = 5.55573396\n",
      "Iteration 30, loss = 5.49436277\n",
      "Iteration 31, loss = 5.42872954\n",
      "Iteration 32, loss = 5.36569446\n",
      "Iteration 33, loss = 5.30739759\n",
      "Iteration 34, loss = 5.25636369\n",
      "Iteration 35, loss = 5.19918800\n",
      "Iteration 36, loss = 5.14330112\n",
      "Iteration 37, loss = 5.09555387\n",
      "Iteration 38, loss = 5.04780074\n",
      "Iteration 39, loss = 5.00189703\n",
      "Iteration 40, loss = 4.95906100\n",
      "Iteration 41, loss = 4.90613478\n",
      "Iteration 42, loss = 4.86939749\n",
      "Iteration 43, loss = 4.83080883\n",
      "Iteration 44, loss = 4.79074149\n",
      "Iteration 45, loss = 4.75008888\n",
      "Iteration 46, loss = 4.71969177\n",
      "Iteration 47, loss = 4.68340897\n",
      "Iteration 48, loss = 4.65170684\n",
      "Iteration 49, loss = 4.62336594\n",
      "Iteration 50, loss = 4.59132880\n",
      "Iteration 51, loss = 4.56188860\n",
      "Iteration 52, loss = 4.53190691\n",
      "Iteration 53, loss = 4.50541306\n",
      "Iteration 54, loss = 4.47523651\n",
      "Iteration 55, loss = 4.45206155\n",
      "Iteration 56, loss = 4.42788028\n",
      "Iteration 57, loss = 4.39907131\n",
      "Iteration 58, loss = 4.38378140\n",
      "Iteration 59, loss = 4.35873345\n",
      "Iteration 60, loss = 4.33887521\n",
      "Iteration 61, loss = 4.31837879\n",
      "Iteration 62, loss = 4.29679921\n",
      "Iteration 63, loss = 4.27743581\n",
      "Iteration 64, loss = 4.26080849\n",
      "Iteration 65, loss = 4.24418247\n",
      "Iteration 66, loss = 4.22626475\n",
      "Iteration 67, loss = 4.21392558\n",
      "Iteration 68, loss = 4.19461192\n",
      "Iteration 69, loss = 4.17894381\n",
      "Iteration 70, loss = 4.16427020\n",
      "Iteration 71, loss = 4.15099590\n",
      "Iteration 72, loss = 4.13084897\n",
      "Iteration 73, loss = 4.12171389\n",
      "Iteration 74, loss = 4.10654065\n",
      "Iteration 75, loss = 4.09381938\n",
      "Iteration 76, loss = 4.08082503\n",
      "Iteration 77, loss = 4.06988448\n",
      "Iteration 78, loss = 4.05584047\n",
      "Iteration 79, loss = 4.04291690\n",
      "Iteration 80, loss = 4.03501429\n"
     ]
    }
   ],
   "source": [
    "crossValidation(pad1000_1DToneOriginalData, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Next steps:</b>\n",
    "\n",
    "<b>Deciding the kind of data we will input into the classifier</b>\n",
    "\n",
    "<b>Reevaluating the data type and classifier choices</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
